"""ter Braak (1992) — residual permutation under the reduced model.

The core idea: under H₀(j) “β_j = 0”, the response is generated by
all predictors *except* X_j.  We therefore:

1.  Fit the **reduced model** Y ~ X_{−j} to get predicted values
    ŷ₋ⱼ and residuals e₋ⱼ = Y − ŷ₋ⱼ.  Under H₀ the residuals are
    exchangeable (identically distributed, weakly dependent).

2.  **Permute** the residuals: π(e₋ⱼ).

3.  **Reconstruct** a synthetic response: Y* = ŷ₋ⱼ + π(e₋ⱼ).  This
    preserves the signal from X_{−j} while destroying any real
    contribution of X_j.

4.  **Refit** the full model on (X, Y*) and extract β*_j.

Repeating for B permutations gives the null distribution of β*_j.

Two code paths:

* **Residual path** — standard residual-permutation for families with
  well-defined residuals (linear, logistic, Poisson, negative binomial).
* **Direct Y permutation (Manly 1997)** — for families where residuals
  are ill-defined (e.g. ordinal, multinomial).  Controlled by
  ``family.direct_permutation``.

Reference:
    ter Braak, C. J. F. (1992). Permutation versus bootstrap
    significance tests in multiple regression and ANOVA.
    In K.-H. Jöckel et al. (Eds.), *Bootstrapping and Related
    Techniques* (pp. 79–86). Springer.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd

from ..families import fit_reduced

if TYPE_CHECKING:
    from ..families import ModelFamily


class TerBraakStrategy:
    """Residual-permutation strategy (ter Braak 1992 / Manly 1997).

    Individual test: returns ``np.ndarray`` of shape ``(B, n_features)``.
    """

    is_joint: bool = False

    def execute(
        self,
        X: pd.DataFrame,
        y_values: np.ndarray,
        family: ModelFamily,
        perm_indices: np.ndarray,
        *,
        confounders: list[str] | None = None,
        model_coefs: np.ndarray | None = None,
        fit_intercept: bool = True,
        n_jobs: int = 1,
    ) -> np.ndarray:
        """Run the ter Braak permutation algorithm.

        Args:
            X: Feature matrix as a pandas DataFrame.
            y_values: Response vector of shape ``(n,)``.
            family: Resolved ``ModelFamily`` instance.
            perm_indices: Pre-generated permutation indices ``(B, n)``.
            confounders: Unused (ter Braak does not partition
                confounders).
            model_coefs: Unused.
            fit_intercept: Whether to include an intercept.
            n_jobs: Parallelism level for the batch-fit step.

        Returns:
            Array of shape ``(B, n_features)`` with permuted
            coefficients.
        """
        X_np = X.values.astype(float)  # (n, p) full design matrix
        n_perm, n = perm_indices.shape  # B permutations, n observations
        n_features = X_np.shape[1]  # p = number of predictors

        # --- Direct Y permutation path (Manly 1997) ---
        # For families where residuals are not well-defined (e.g.
        # ordinal, multinomial), permute Y directly rather than going
        # through the residual pipeline.  This is a simpler but less
        # powerful test: it permutes the *entire* response, not just
        # the unexplained part, so it tests marginal rather than
        # partial association.
        if family.direct_permutation:
            # Fancy-index y_values with the (B, n) permutation matrix
            # to produce B shuffled response vectors simultaneously.
            Y_perm = y_values[perm_indices]  # (B, n)
            # batch_fit returns (B, p) coefficient matrix.
            return family.batch_fit(X_np, Y_perm, fit_intercept, n_jobs=n_jobs)

        # --- Standard residual-permutation path (ter Braak 1992) ---
        result = np.zeros((n_perm, n_features))  # (B, p) permuted coefficients

        # Derive a deterministic RNG from the first permutation index
        # so that any stochastic reconstruction step (e.g. Bernoulli
        # sampling for logistic residuals → binary Y) is reproducible
        # given the same permutation matrix.
        rng = np.random.default_rng(int(perm_indices[0, 0]))

        # Loop over each feature j ∈ {0, …, p−1}.  Each iteration
        # tests H₀(j): β_j = 0, i.e. feature j has no effect on Y
        # after controlling for all other features.
        for j in range(n_features):
            # Step 1: Fit the reduced model Y ~ X_{−j}.
            # np.delete removes column j, yielding (n, p−1) design.
            X_red = np.delete(X_np, j, axis=1)  # (n, p−1)
            # fit_reduced returns (model_or_None, predicted_values).
            # model is None when X_red has zero columns (all features
            # are the one being tested).
            reduced_model, preds_red = fit_reduced(
                family, X_red, y_values, fit_intercept
            )  # preds_red = ŷ₋ⱼ, shape (n,)

            # Residuals from the reduced model: e₋ⱼ = Y − ŷ₋ⱼ.
            # For GLM families, family.residuals() computes the
            # appropriate residual type (response residuals for linear,
            # deviance or working residuals for others).
            if reduced_model is not None:
                resids_red = family.residuals(reduced_model, X_red, y_values)
            else:
                # Zero-column edge case: no predictors, so the
                # prediction is the intercept (mean) and residuals
                # are raw: e = Y − ŷ.
                resids_red = y_values - preds_red  # (n,)

            # Step 2: Permute the residual vector.
            # Fancy-indexing with (B, n) indices broadcasts the 1-D
            # residual array into B shuffled copies.
            permuted_resids = resids_red[perm_indices]  # (B, n)

            # Step 3: Reconstruct permuted response vectors.
            # Y* = ŷ₋ⱼ + π(e₋ⱼ) for linear family.
            # For logistic, reconstruct_y clamps to [0,1] and draws
            # Bernoulli(p*) to get binary Y*.  For Poisson/NB,
            # applies exp() link inverse.  The np.newaxis broadcast
            # lets (1, n) predictions combine with (B, n) residuals.
            Y_perm = family.reconstruct_y(
                preds_red[np.newaxis, :],  # (1, n) → broadcast to (B, n)
                permuted_resids,  # (B, n)
                rng,
            )  # (B, n) synthetic response vectors

            # Step 4: Batch-refit the full model on all B synthetic
            # responses.  Returns (B, p) coefficient matrix.
            all_coefs = family.batch_fit(
                X_np, Y_perm, fit_intercept, n_jobs=n_jobs
            )  # (B, p)
            # Extract only column j — the coefficient under test.
            # Other columns test different hypotheses and belong to
            # other iterations of this loop.
            result[:, j] = all_coefs[:, j]

        return result
